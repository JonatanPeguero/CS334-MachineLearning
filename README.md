üìò CS334 ‚Äì Machine Learning (Spring 2025)
University: Emory University
Language: Python
Topics: Supervised Learning, CNNs, Bandits, Clustering, Recommender Systems

üêß HW1 ‚Äì Data Visualization with Penguins Dataset
Summary:

Introduced to Python-based data science workflows using the palmerpenguins dataset. The focus was on data cleaning, exploratory data analysis, visualization, and basic scripting.

Key Tasks:

Cleaned CSV-formatted penguin data with missing values

Wrote Python scripts for generating histograms, boxplots, and scatterplots

Used matplotlib and pandas to explore distributions and relationships between features

Created visualizations of body mass, flipper length, and bill measurements

Implemented a sum of squared errors calculator (sumsquare.py)

Skills: pandas, matplotlib, data cleaning, plotting, script modularity

üìà HW2 ‚Äì Linear Regression and Regularization
Summary:

Implemented linear regression from scratch using batch gradient descent, with optional regularization (L1 and L2). Evaluated learning curves and effects of hyperparameters.

Key Tasks:

Built a modular linear regression class with support for Lasso and Ridge

Explored weight sparsity under L1 vs L2 penalties

Trained on synthetic data to visualize underfitting, overfitting, and convergence behavior

Generated plots of validation loss vs regularization strength

Skills: NumPy, vectorization, optimization, regularization, gradient descent

üß† HW3 ‚Äì Logistic Regression and Classification
Summary:

Developed logistic regression for binary classification tasks, emphasizing proper loss function implementation, gradient checking, and model evaluation.

Key Tasks:

Implemented logistic regression using log-likelihood loss

Visualized class boundaries and confidence scores

Explored L0, L1, and L2 penalty effects on feature selection

Used 2D scatter plots and loss curves to analyze regularization

Skills: Classification, NumPy, plotting, regularization comparisons

üå≤ HW4 ‚Äì Decision Trees, Bagging, and Boosting
Summary:

Implemented decision trees from scratch and evaluated ensemble methods like random forests and AdaBoost. Emphasized generalization and the tradeoff between depth and performance.

Key Tasks:

Built a decision tree classifier supporting Gini impurity and entropy

Implemented pruning to avoid overfitting

Compared bagging (random forests) vs boosting (AdaBoost)

Visualized accuracy and tree depth over time

Skills: Recursion, ensemble methods, decision boundaries, model interpretability

üê∂ HW5 ‚Äì Convolutional Neural Networks for Image Classification
Summary:

Trained a CNN using PyTorch to classify 10 dog breeds using a labeled image dataset. Included data preprocessing, custom model architecture, training loops, and visualization.

Key Tasks:

Created and trained a 6-layer CNN using custom data loaders

Analyzed training/validation accuracy and loss

Visualized learned convolution filters and activations

Designed a challenge model for better performance

Skills: PyTorch, CNNs, training loops, transfer learning, image preprocessing

üé∞ HW6 ‚Äì Multi-Armed Bandits, Matrix Factorization, K-Means Clustering
Summary:

A 3-part assignment covering reinforcement learning (multi-armed bandits), recommender systems (collaborative filtering), and unsupervised learning (K-means clustering).

Key Tasks:

Implemented Œµ-greedy, optimistic initialization, and UCB bandit strategies

Trained a matrix factorization model for course recommendations

Ran and analyzed K-means with different tie-breaking strategies

Skills: Reinforcement learning, recommender systems, unsupervised learning, simulation

